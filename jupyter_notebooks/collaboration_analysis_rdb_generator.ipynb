{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18aa0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from networkx.algorithms.centrality import betweenness_centrality, degree_centrality, eigenvector_centrality, closeness_centrality\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafefcd",
   "metadata": {},
   "source": [
    "## Read Transcription Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80651084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all transcription files\n",
    "def read_transcriptions(folder_path):\n",
    "    files = sorted([f for f in os.listdir(folder_path)\n",
    "                   if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    transcriptions = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            transcriptions.append(f.read())\n",
    "    return transcriptions\n",
    "\n",
    "\n",
    "project3_path = 'transcriptions/project3'\n",
    "project4_path = 'transcriptions/project4'\n",
    "transcriptions_project3 = read_transcriptions(project3_path)\n",
    "transcriptions_project4 = read_transcriptions(project4_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed7b0e",
   "metadata": {},
   "source": [
    "## Extract Speaker Turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81fff65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract speaker turns and calculate word counts\n",
    "def extract_speaker_turns(data):\n",
    "    sections = data.split('\\n\\n')  # Split sections by double new lines\n",
    "    speakers = []\n",
    "    texts = []\n",
    "    word_counts = []\n",
    "    for section in sections:\n",
    "        lines = section.strip().split('\\n')\n",
    "        if len(lines) > 1:\n",
    "            speaker_line = lines[0]\n",
    "            text_lines = lines[1:]\n",
    "            speaker_match = re.search(r'Speaker (SPEAKER_\\d+)', speaker_line)\n",
    "            if speaker_match:\n",
    "                speakers.append(speaker_match.group(1))\n",
    "                text = ' '.join(text_lines)\n",
    "                texts.append(text)\n",
    "                word_counts.append(len(text.split()))\n",
    "    df = pd.DataFrame({'Speaker': speakers, 'Text': texts,\n",
    "                      'Word_Count': word_counts})\n",
    "    return df\n",
    "\n",
    "\n",
    "dfs_project3 = [extract_speaker_turns(data)\n",
    "                for data in transcriptions_project3]\n",
    "dfs_project4 = [extract_speaker_turns(data)\n",
    "                for data in transcriptions_project4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d2e21",
   "metadata": {},
   "source": [
    "## Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb020c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataset\n",
    "def create_dataset(dfs, project_number):\n",
    "    dataset = []\n",
    "    for i, df in enumerate(dfs):\n",
    "        df['meeting_number'] = i + 1  # Add meeting number\n",
    "        df['project'] = project_number  # Add project number\n",
    "        speaker_word_counts = df.groupby(\n",
    "            'Speaker')['Word_Count'].sum().to_dict()\n",
    "        total_words = df['Word_Count'].sum()\n",
    "        for speaker, word_count in speaker_word_counts.items():\n",
    "            dataset.append({\n",
    "                'id': f'{project_number}_{i+1}_{speaker}',\n",
    "                'project': project_number,\n",
    "                'meeting_number': i + 1,\n",
    "                'speaker_number': int(speaker.split('_')[1]),\n",
    "                'speech_frequency': word_count,\n",
    "                'total_words': total_words\n",
    "            })\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "\n",
    "dataset_project3 = create_dataset(dfs_project3, 3)\n",
    "dataset_project4 = create_dataset(dfs_project4, 4)\n",
    "dataset = pd.concat([dataset_project3, dataset_project4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafd5dc",
   "metadata": {},
   "source": [
    "## Create Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0e12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_time_in_minutes(text):\n",
    "    time_pattern_hms = re.compile(r'\\b\\d{1,2}:\\d{2}:\\d{2}\\b')\n",
    "    time_pattern_ms = re.compile(r'\\b\\d{2}:\\d{2}\\b')\n",
    "    times_hms = time_pattern_hms.findall(text)\n",
    "    times_ms = time_pattern_ms.findall(text)\n",
    "\n",
    "    if not times_hms and not times_ms:\n",
    "        return None\n",
    "    if times_hms:\n",
    "        last_time = times_hms[-1]\n",
    "        hours, minutes, seconds = map(int, last_time.split(':'))\n",
    "    else:\n",
    "        last_time = times_ms[-1]\n",
    "        hours = 0\n",
    "        minutes, seconds = map(int, last_time.split(':'))\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    if seconds > 0:\n",
    "        total_minutes += math.ceil(seconds / 60)\n",
    "\n",
    "    return total_minutes\n",
    "\n",
    "\n",
    "def process_files_in_directory(directory_path):\n",
    "    durations = []\n",
    "    files = os.listdir(directory_path)\n",
    "    txt_files = sorted([file for file in files if file.endswith('.txt')])\n",
    "    for filename in txt_files:\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        minutes = extract_last_time_in_minutes(text)\n",
    "        if minutes is not None:\n",
    "            durations.append(minutes)\n",
    "    return durations\n",
    "\n",
    "\n",
    "project3_durations = process_files_in_directory(project3_path)\n",
    "project4_durations = process_files_in_directory(project4_path)\n",
    "\n",
    "duration_data = {\n",
    "    3: project3_durations,\n",
    "    4: project4_durations\n",
    "}\n",
    "\n",
    "durations = []\n",
    "for project, durations_list in duration_data.items():\n",
    "    for meeting_num, duration in enumerate(durations_list, start=1):\n",
    "        durations.extend([duration] * len(dataset[(dataset['project']\n",
    "                         == project) & (dataset['meeting_number'] == meeting_num)]))\n",
    "\n",
    "dataset['duration'] = durations\n",
    "dataset['duration'] = dataset['duration'] / \\\n",
    "    60  # Convert duration from minutes to hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da8669",
   "metadata": {},
   "source": [
    "## Normalize Speech Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150e5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize speech frequency as speech_frequency / duration\n",
    "dataset['normalized_speech_frequency'] = dataset['speech_frequency'] / \\\n",
    "    dataset['duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e3957",
   "metadata": {},
   "source": [
    "## Compute Interaction Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53735b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Interaction Frequency\n",
    "def compute_interaction_frequency(df, project_number):\n",
    "    interaction_counts = defaultdict(lambda: defaultdict(int))\n",
    "    interaction_records = []\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        interaction_counts[prev_speaker][next_speaker] += 1\n",
    "    for prev_speaker, next_speakers in interaction_counts.items():\n",
    "        for next_speaker, count in next_speakers.items():\n",
    "            interaction_records.append({\n",
    "                'project': project_number,\n",
    "                'meeting_number': df['meeting_number'].iloc[0],\n",
    "                'speaker_id': int(prev_speaker.split('_')[1]),\n",
    "                'next_speaker_id': int(next_speaker.split('_')[1]),\n",
    "                'count': count\n",
    "            })\n",
    "    return pd.DataFrame(interaction_records)\n",
    "\n",
    "\n",
    "interaction_records_project3 = pd.concat(\n",
    "    [compute_interaction_frequency(df, 3) for df in dfs_project3], ignore_index=True)\n",
    "interaction_records_project4 = pd.concat(\n",
    "    [compute_interaction_frequency(df, 4) for df in dfs_project4], ignore_index=True)\n",
    "interaction_records = pd.concat(\n",
    "    [interaction_records_project3, interaction_records_project4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4115e01",
   "metadata": {},
   "source": [
    "## Generate All Possible Speaker Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34615e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible speaker pairs for each meeting and fill in missing combinations with zeros\n",
    "def generate_all_pairs(interaction_records, dataset):\n",
    "    all_pairs = []\n",
    "    for (project, meeting), group in dataset.groupby(['project', 'meeting_number']):\n",
    "        speakers = group['speaker_number'].unique()\n",
    "        for speaker1 in speakers:\n",
    "            for speaker2 in speakers:\n",
    "                if not interaction_records[(interaction_records['project'] == project) & (interaction_records['meeting_number'] == meeting) & (interaction_records['speaker_id'] == speaker1) & (interaction_records['next_speaker_id'] == speaker2)].empty:\n",
    "                    continue\n",
    "                all_pairs.append({\n",
    "                    'project': project,\n",
    "                    'meeting_number': meeting,\n",
    "                    'speaker_id': speaker1,\n",
    "                    'next_speaker_id': speaker2,\n",
    "                    'count': 0\n",
    "                })\n",
    "    return pd.DataFrame(all_pairs)\n",
    "\n",
    "\n",
    "all_pairs = generate_all_pairs(interaction_records, dataset)\n",
    "interaction_records = pd.concat(\n",
    "    [interaction_records, all_pairs], ignore_index=True)\n",
    "interaction_records = interaction_records.sort_values(\n",
    "    by=['project', 'meeting_number', 'speaker_id', 'next_speaker_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22127420",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0152ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "combined_dataset = pd.merge(dataset, interaction_records, how='left', left_on=[\n",
    "                            'project', 'meeting_number', 'speaker_number'], right_on=['project', 'meeting_number', 'speaker_id'])\n",
    "combined_dataset['count'] = combined_dataset['count'].fillna(0).astype(int)\n",
    "combined_dataset['normalized_weight'] = combined_dataset['count'] / \\\n",
    "    combined_dataset['duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57659c65",
   "metadata": {},
   "source": [
    "## Compute Network Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c12541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network density function\n",
    "def compute_density(G):\n",
    "    num_nodes = len(G)\n",
    "    if num_nodes < 2:\n",
    "        return 0\n",
    "    possible_edges = num_nodes * (num_nodes - 1)  # For directed graph\n",
    "    actual_edges = sum(1 for u, v, data in G.edges(\n",
    "        data=True) if u != v and data['weight'] > 0)\n",
    "    return actual_edges / possible_edges\n",
    "\n",
    "\n",
    "# Compute density\n",
    "densities_project3 = []\n",
    "densities_project4 = []\n",
    "\n",
    "for df in dfs_project3:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            # Self-interaction if last speaker\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            combined_row = combined_dataset[(combined_dataset['project'] == df['project'].iloc[i]) &\n",
    "                                            (combined_dataset['meeting_number'] == df['meeting_number'].iloc[i]) &\n",
    "                                            (combined_dataset['speaker_number'] == int(prev_speaker.split('_')[1])) &\n",
    "                                            (combined_dataset['next_speaker_id'] == int(next_speaker.split('_')[1]))]\n",
    "            if not combined_row.empty:\n",
    "                normalized_weight = combined_row['normalized_weight'].values[0]\n",
    "                if G.has_edge(prev_speaker, next_speaker):\n",
    "                    G[prev_speaker][next_speaker]['weight'] += normalized_weight\n",
    "                else:\n",
    "                    G.add_edge(prev_speaker, next_speaker,\n",
    "                               weight=normalized_weight)\n",
    "    densities_project3.append(compute_density(G))\n",
    "\n",
    "for df in dfs_project4:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            # Self-interaction if last speaker\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            combined_row = combined_dataset[(combined_dataset['project'] == df['project'].iloc[i]) &\n",
    "                                            (combined_dataset['meeting_number'] == df['meeting_number'].iloc[i]) &\n",
    "                                            (combined_dataset['speaker_number'] == int(prev_speaker.split('_')[1])) &\n",
    "                                            (combined_dataset['next_speaker_id'] == int(next_speaker.split('_')[1]))]\n",
    "            if not combined_row.empty:\n",
    "                normalized_weight = combined_row['normalized_weight'].values[0]\n",
    "                if G.has_edge(prev_speaker, next_speaker):\n",
    "                    G[prev_speaker][next_speaker]['weight'] += normalized_weight\n",
    "                else:\n",
    "                    G.add_edge(prev_speaker, next_speaker,\n",
    "                               weight=normalized_weight)\n",
    "    densities_project4.append(compute_density(G))\n",
    "\n",
    "# Define the weighted density function\n",
    "\n",
    "\n",
    "def weighted_density(G):\n",
    "    if len(G) == 0:\n",
    "        return 0\n",
    "    total_weight = sum(data['weight'] for u, v, data in G.edges(data=True))\n",
    "    # total_weight = sum(data['weight'] for u, v, data in G.edges(data=True) if u != v)\n",
    "    num_nodes = len(G)\n",
    "    max_weight = max(data['weight'] for u, v, data in G.edges(data=True))\n",
    "    # max_weight = max(data['weight'] for u, v, data in G.edges(data=True) if u != v)\n",
    "    possible_edges = num_nodes * (num_nodes)  # For directed graph\n",
    "    return total_weight / (possible_edges * max_weight) if possible_edges > 0 else 0\n",
    "\n",
    "\n",
    "# Compute weighted density\n",
    "weighted_densities_project3 = []\n",
    "weighted_densities_project4 = []\n",
    "\n",
    "for df in dfs_project3:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            # Self-interaction if last speaker\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        if df.iloc[i]['Text'].strip() != '':\n",
    "            combined_row = combined_dataset[(combined_dataset['project'] == df['project'].iloc[i]) &\n",
    "                                            (combined_dataset['meeting_number'] == df['meeting_number'].iloc[i]) &\n",
    "                                            (combined_dataset['speaker_number'] == int(prev_speaker.split('_')[1])) &\n",
    "                                            (combined_dataset['next_speaker_id'] == int(next_speaker.split('_')[1]))]\n",
    "            if not combined_row.empty:\n",
    "                normalized_weight = combined_row['normalized_weight'].values[0]\n",
    "                if G.has_edge(prev_speaker, next_speaker):\n",
    "                    G[prev_speaker][next_speaker]['weight'] += normalized_weight\n",
    "                else:\n",
    "                    G.add_edge(prev_speaker, next_speaker,\n",
    "                               weight=normalized_weight)\n",
    "    weighted_densities_project3.append(weighted_density(G))\n",
    "\n",
    "for df in dfs_project4:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            # Self-interaction if last speaker\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        if df.iloc[i]['Text'].strip() != '':\n",
    "            combined_row = combined_dataset[(combined_dataset['project'] == df['project'].iloc[i]) &\n",
    "                                            (combined_dataset['meeting_number'] == df['meeting_number'].iloc[i]) &\n",
    "                                            (combined_dataset['speaker_number'] == int(prev_speaker.split('_')[1])) &\n",
    "                                            (combined_dataset['next_speaker_id'] == int(next_speaker.split('_')[1]))]\n",
    "            if not combined_row.empty:\n",
    "                normalized_weight = combined_row['normalized_weight'].values[0]\n",
    "                if G.has_edge(prev_speaker, next_speaker):\n",
    "                    G[prev_speaker][next_speaker]['weight'] += normalized_weight\n",
    "                else:\n",
    "                    G.add_edge(prev_speaker, next_speaker,\n",
    "                               weight=normalized_weight)\n",
    "    weighted_densities_project4.append(weighted_density(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0671a9b",
   "metadata": {},
   "source": [
    "## Compute Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550dad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define centrality measures function\n",
    "def compute_centralities(df):\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            # Self-interaction if last speaker\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            combined_row = combined_dataset[(combined_dataset['project'] == df['project'].iloc[i]) & (combined_dataset['meeting_number'] == df['meeting_number'].iloc[i]) & (\n",
    "                combined_dataset['speaker_number'] == int(prev_speaker.split('_')[1])) & (combined_dataset['next_speaker_id'] == int(next_speaker.split('_')[1]))]\n",
    "            if not combined_row.empty:\n",
    "                weight = combined_row['normalized_weight'].values[0]\n",
    "                if G.has_edge(prev_speaker, next_speaker):\n",
    "                    G[prev_speaker][next_speaker]['weight'] += weight\n",
    "                else:\n",
    "                    G.add_edge(prev_speaker, next_speaker, weight=weight)\n",
    "    if len(G) == 0:\n",
    "        centralities = {\n",
    "            'degree_centrality': {},\n",
    "            'indegree_centrality': {},\n",
    "            'outdegree_centrality': {},\n",
    "            'betweenness_centrality': {},\n",
    "            'closeness_centrality': {},\n",
    "            'eigenvector_centrality': {},\n",
    "            'pagerank': {}\n",
    "        }\n",
    "    else:\n",
    "        try:\n",
    "            eigenvector_cent = eigenvector_centrality(\n",
    "                G, max_iter=2000, weight='weight')\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            eigenvector_cent = {node: 0 for node in G.nodes()}\n",
    "        num_nodes = len(G)\n",
    "        possible_edges = num_nodes * (num_nodes - 1)\n",
    "        # possible_edges = num_nodes * (num_nodes)\n",
    "        max_weight = max(data['weight']\n",
    "                         for u, v, data in G.edges(data=True) if u != v)\n",
    "        # max_weight = max(data['weight'] for u, v, data in G.edges(data=True))\n",
    "        centralities = {\n",
    "            'degree_centrality': {k: v / (possible_edges * max_weight) for k, v in dict(G.degree(weight='weight')).items()},\n",
    "            'indegree_centrality': {k: v / (possible_edges * max_weight) for k, v in dict(G.in_degree(weight='weight')).items()},\n",
    "            'outdegree_centrality': {k: v / (possible_edges * max_weight) for k, v in dict(G.out_degree(weight='weight')).items()},\n",
    "            'betweenness_centrality': {k: v / (possible_edges * max_weight) for k, v in betweenness_centrality(G, weight='weight').items()},\n",
    "            'closeness_centrality': {k: v / (possible_edges * max_weight) for k, v in closeness_centrality(G, distance='weight').items()},\n",
    "            'eigenvector_centrality': {k: v / (possible_edges * max_weight) for k, v in eigenvector_cent.items()},\n",
    "            'pagerank': {k: v / (possible_edges * max_weight) for k, v in nx.pagerank(G, weight='weight').items()}\n",
    "        }\n",
    "    return centralities\n",
    "\n",
    "\n",
    "centralities_project3 = []\n",
    "centralities_project4 = []\n",
    "for df in dfs_project3:\n",
    "    centralities_project3.append(compute_centralities(df))\n",
    "\n",
    "for df in dfs_project4:\n",
    "    centralities_project4.append(compute_centralities(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3912ac",
   "metadata": {},
   "source": [
    "## Add Centralities and Network Density to Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ecd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Centralities and Density to Combined Dataset\n",
    "for centrality_measure in ['degree_centrality', 'indegree_centrality', 'outdegree_centrality', 'betweenness_centrality', 'closeness_centrality', 'eigenvector_centrality', 'pagerank']:\n",
    "    combined_dataset[centrality_measure] = 0\n",
    "combined_dataset['network_density'] = 0\n",
    "combined_dataset['weighted_network_density'] = 0\n",
    "\n",
    "for i, df in enumerate(dfs_project3):\n",
    "    centralities = centralities_project3[i]\n",
    "    density = densities_project3[i]\n",
    "    weighted_density_value = weighted_densities_project3[i]\n",
    "    for centrality_measure, centrality_values in centralities.items():\n",
    "        for node, value in centrality_values.items():\n",
    "            combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1) & (\n",
    "                combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'network_density'] = density\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'weighted_network_density'] = weighted_density_value\n",
    "\n",
    "for i, df in enumerate(dfs_project4):\n",
    "    centralities = centralities_project4[i]\n",
    "    density = densities_project4[i]\n",
    "    weighted_density_value = weighted_densities_project4[i]\n",
    "    for centrality_measure, centrality_values in centralities.items():\n",
    "        for node, value in centrality_values.items():\n",
    "            combined_dataset.loc[(combined_dataset['project'] == 4) & (combined_dataset['meeting_number'] == i + 1) & (\n",
    "                combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'network_density'] = density\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'weighted_network_density'] = weighted_density_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928663b3",
   "metadata": {},
   "source": [
    "## Compute Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e4c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gini coefficient function\n",
    "def gini_coefficient(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    if np.amin(x) < 0:\n",
    "        x -= np.amin(x)  # values cannot be negative\n",
    "    x += 0.0000001  # values cannot be 0\n",
    "    x = np.sort(x)  # values must be sorted\n",
    "    index = np.arange(1, x.shape[0] + 1)  # index per array element\n",
    "    n = x.shape[0]\n",
    "    return ((np.sum((2 * index - n - 1) * x)) / (n * np.sum(x)))\n",
    "\n",
    "# Compute Gini Coefficient for each meeting\n",
    "\n",
    "\n",
    "def compute_gini(df):\n",
    "    gini_values = []\n",
    "    meetings = df['meeting_number'].unique()\n",
    "    for meeting_number in meetings:\n",
    "        meeting_data = df[df['meeting_number'] == meeting_number]\n",
    "        interaction_counts = [meeting_data[meeting_data['speaker_number'] == speaker]['count'].sum(\n",
    "        ) for speaker in meeting_data['speaker_number'].unique()]\n",
    "        # interaction_counts = [meeting_data[(meeting_data['speaker_number'] == speaker) & (meeting_data['speaker_number'] != meeting_data['next_speaker_id'])]['count'].sum() for speaker in meeting_data['speaker_number'].unique()]\n",
    "        gini_values.append(gini_coefficient(interaction_counts))\n",
    "    return gini_values\n",
    "\n",
    "\n",
    "gini_project3 = compute_gini(\n",
    "    combined_dataset[combined_dataset['project'] == 3])\n",
    "gini_project4 = compute_gini(\n",
    "    combined_dataset[combined_dataset['project'] == 4])\n",
    "\n",
    "combined_dataset['gini_coefficient'] = 0\n",
    "\n",
    "for i in range(len(gini_project3)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'gini_coefficient'] = gini_project3[i]\n",
    "\n",
    "for i in range(len(gini_project4)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'gini_coefficient'] = gini_project4[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91da12",
   "metadata": {},
   "source": [
    "## Compute Interaction Equality Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd71b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Interaction Equality Index function\n",
    "def interaction_equality_index(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    mean_x = np.mean(x)\n",
    "    if mean_x == 0:\n",
    "        return 0\n",
    "    return 1 - (np.std(x) / mean_x)\n",
    "\n",
    "# Compute Interaction Equality Index for each meeting\n",
    "\n",
    "\n",
    "def compute_equality_index(df):\n",
    "    equality_index_values = []\n",
    "    meetings = df['meeting_number'].unique()\n",
    "    for meeting_number in meetings:\n",
    "        meeting_data = df[df['meeting_number'] == meeting_number]\n",
    "        interaction_counts = [meeting_data[meeting_data['speaker_number'] == speaker]['count'].sum(\n",
    "        ) for speaker in meeting_data['speaker_number'].unique()]\n",
    "        # interaction_counts = [meeting_data[(meeting_data['speaker_number'] == speaker) & (meeting_data['speaker_number'] != meeting_data['next_speaker_id'])]['count'].sum() for speaker in meeting_data['speaker_number'].unique()]\n",
    "        equality_index_values.append(\n",
    "            interaction_equality_index(interaction_counts))\n",
    "    return equality_index_values\n",
    "\n",
    "\n",
    "equality_index_project3 = compute_equality_index(\n",
    "    combined_dataset[combined_dataset['project'] == 3])\n",
    "equality_index_project4 = compute_equality_index(\n",
    "    combined_dataset[combined_dataset['project'] == 4])\n",
    "\n",
    "combined_dataset['interaction_equality_index'] = 0\n",
    "\n",
    "for i in range(len(equality_index_project3)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'interaction_equality_index'] = equality_index_project3[i]\n",
    "\n",
    "for i in range(len(equality_index_project4)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (\n",
    "        combined_dataset['meeting_number'] == i + 1), 'interaction_equality_index'] = equality_index_project4[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f322e",
   "metadata": {},
   "source": [
    "## Save Updated Combined Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e876d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "columns_order = [\n",
    "    'id', 'project', 'meeting_number', 'speaker_number', 'speech_frequency', 'total_words', 'duration', 'normalized_speech_frequency', 'speaker_id', 'next_speaker_id', 'count', 'normalized_weight', 'network_density', 'weighted_network_density',\n",
    "    'gini_coefficient', 'interaction_equality_index', 'degree_centrality', 'indegree_centrality', 'outdegree_centrality', 'betweenness_centrality', 'closeness_centrality', 'eigenvector_centrality', 'pagerank'\n",
    "]\n",
    "combined_dataset = combined_dataset[columns_order]\n",
    "\n",
    "# Save the final dataset with centralities and density to a CSV file\n",
    "os.makedirs('data', exist_ok=True)\n",
    "combined_dataset.to_csv('data/dataset_collaboration.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
