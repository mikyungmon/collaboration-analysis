{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d0c6c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from networkx.algorithms.centrality import betweenness_centrality, degree_centrality, eigenvector_centrality, closeness_centrality\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ab379",
   "metadata": {},
   "source": [
    "## Read Transcription Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "63d03b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all transcription files\n",
    "def read_transcriptions(folder_path):\n",
    "    files = sorted([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "    transcriptions = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:\n",
    "            transcriptions.append(f.read())\n",
    "    return transcriptions\n",
    "\n",
    "project3_path = 'transcriptions/project3'\n",
    "project4_path = 'transcriptions/project4'\n",
    "transcriptions_project3 = read_transcriptions(project3_path)\n",
    "transcriptions_project4 = read_transcriptions(project4_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3949db",
   "metadata": {},
   "source": [
    "## Extract Speaker Turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eb9fdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract speaker turns and calculate word counts\n",
    "def extract_speaker_turns(data):\n",
    "    sections = data.split('\\n\\n')  # Split sections by double new lines\n",
    "    speakers = []\n",
    "    texts = []\n",
    "    word_counts = []\n",
    "    for section in sections:\n",
    "        lines = section.strip().split('\\n')\n",
    "        if len(lines) > 1:\n",
    "            speaker_line = lines[0]\n",
    "            text_lines = lines[1:]\n",
    "            speaker_match = re.search(r'Speaker (SPEAKER_\\d+)', speaker_line)\n",
    "            if speaker_match:\n",
    "                speakers.append(speaker_match.group(1))\n",
    "                text = ' '.join(text_lines)\n",
    "                texts.append(text)\n",
    "                word_counts.append(len(text.split()))\n",
    "    df = pd.DataFrame({'Speaker': speakers, 'Text': texts, 'Word_Count': word_counts})\n",
    "    return df\n",
    "\n",
    "dfs_project3 = [extract_speaker_turns(data) for data in transcriptions_project3]\n",
    "dfs_project4 = [extract_speaker_turns(data) for data in transcriptions_project4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140544f",
   "metadata": {},
   "source": [
    "## Create Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "92f29875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataset\n",
    "def create_dataset(dfs, project_number):\n",
    "    dataset = []\n",
    "    for i, df in enumerate(dfs):\n",
    "        df['meeting_number'] = i + 1  # Add meeting number\n",
    "        speaker_word_counts = df.groupby('Speaker')['Word_Count'].sum().to_dict()\n",
    "        total_words = df['Word_Count'].sum()\n",
    "        for speaker, word_count in speaker_word_counts.items():\n",
    "            dataset.append({\n",
    "                'id': f'{project_number}_{i}_{speaker}',\n",
    "                'project': project_number,\n",
    "                'meeting_number': i + 1,\n",
    "                'speaker_number': int(speaker.split('_')[1]),\n",
    "                'speech_frequency': word_count,\n",
    "                'total_words': total_words\n",
    "            })\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "dataset_project3 = create_dataset(dfs_project3, 3)\n",
    "dataset_project4 = create_dataset(dfs_project4, 4)\n",
    "dataset = pd.concat([dataset_project3, dataset_project4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b1dc0",
   "metadata": {},
   "source": [
    "## Create Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6da0b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_time_in_minutes(text):\n",
    "    time_pattern_hms = re.compile(r'\\b\\d{1,2}:\\d{2}:\\d{2}\\b')\n",
    "    time_pattern_ms = re.compile(r'\\b\\d{2}:\\d{2}\\b')\n",
    "    times_hms = time_pattern_hms.findall(text)\n",
    "    times_ms = time_pattern_ms.findall(text)\n",
    "\n",
    "    if not times_hms and not times_ms:\n",
    "        return None\n",
    "    if times_hms:\n",
    "        last_time = times_hms[-1]\n",
    "        hours, minutes, seconds = map(int, last_time.split(':'))\n",
    "    else:\n",
    "        last_time = times_ms[-1]\n",
    "        hours = 0\n",
    "        minutes, seconds = map(int, last_time.split(':'))\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    if seconds > 0:\n",
    "        total_minutes += math.ceil(seconds / 60)\n",
    "\n",
    "    return total_minutes\n",
    "\n",
    "def process_files_in_directory(directory_path):\n",
    "    durations = []\n",
    "    files = os.listdir(directory_path)\n",
    "    txt_files = sorted([file for file in files if file.endswith('.txt')])\n",
    "    for filename in txt_files:\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        minutes = extract_last_time_in_minutes(text)\n",
    "        if minutes is not None:\n",
    "            durations.append(minutes)\n",
    "    return durations\n",
    "\n",
    "project3_durations = process_files_in_directory(project3_path)\n",
    "project4_durations = process_files_in_directory(project4_path)\n",
    "\n",
    "duration_data = {\n",
    "    3: project3_durations,\n",
    "    4: project4_durations\n",
    "}\n",
    "\n",
    "durations = []\n",
    "for project, durations_list in duration_data.items():\n",
    "    for meeting_num, duration in enumerate(durations_list, start=1):\n",
    "        durations.extend([duration] * len(dataset[(dataset['project'] == project) & (dataset['meeting_number'] == meeting_num)]))\n",
    "\n",
    "dataset['duration'] = durations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed03a1",
   "metadata": {},
   "source": [
    "## Normalize Speech Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ded2bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize speech frequency as speech_frequency / duration\n",
    "dataset['normalized_speech_frequency'] = dataset['speech_frequency'] / dataset['duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc90e39",
   "metadata": {},
   "source": [
    "## Compute Interaction Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "108a6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Interaction Frequency\n",
    "def compute_interaction_frequency(df, project_number):\n",
    "    interaction_counts = defaultdict(lambda: defaultdict(int))\n",
    "    interaction_records = []\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']\n",
    "        interaction_counts[prev_speaker][next_speaker] += 1\n",
    "    for prev_speaker, next_speakers in interaction_counts.items():\n",
    "        for next_speaker, count in next_speakers.items():\n",
    "            interaction_records.append({\n",
    "                'project': project_number,\n",
    "                'meeting_number': df['meeting_number'].iloc[0],\n",
    "                'speaker_id': int(prev_speaker.split('_')[1]),\n",
    "                'next_speaker_id': int(next_speaker.split('_')[1]),\n",
    "                'count': count\n",
    "            })\n",
    "    return pd.DataFrame(interaction_records)\n",
    "\n",
    "interaction_records_project3 = pd.concat([compute_interaction_frequency(df, 3) for df in dfs_project3], ignore_index=True)\n",
    "interaction_records_project4 = pd.concat([compute_interaction_frequency(df, 4) for df in dfs_project4], ignore_index=True)\n",
    "interaction_records = pd.concat([interaction_records_project3, interaction_records_project4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5be07",
   "metadata": {},
   "source": [
    "## Generate All Possible Speaker Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "22b615b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible speaker pairs for each meeting and fill in missing combinations with zeros\n",
    "def generate_all_pairs(interaction_records, dataset):\n",
    "    all_pairs = []\n",
    "    for (project, meeting), group in dataset.groupby(['project', 'meeting_number']):\n",
    "        speakers = group['speaker_number'].unique()\n",
    "        for speaker1 in speakers:\n",
    "            for speaker2 in speakers:\n",
    "                if not interaction_records[(interaction_records['project'] == project) & (interaction_records['meeting_number'] == meeting) & (interaction_records['speaker_id'] == speaker1) & (interaction_records['next_speaker_id'] == speaker2)].empty:\n",
    "                    continue\n",
    "                all_pairs.append({\n",
    "                    'project': project,\n",
    "                    'meeting_number': meeting,\n",
    "                    'speaker_id': speaker1,\n",
    "                    'next_speaker_id': speaker2,\n",
    "                    'count': 0\n",
    "                })\n",
    "    return pd.DataFrame(all_pairs)\n",
    "\n",
    "all_pairs = generate_all_pairs(interaction_records, dataset)\n",
    "interaction_records = pd.concat([interaction_records, all_pairs], ignore_index=True)\n",
    "interaction_records = interaction_records.sort_values(by=['project', 'meeting_number', 'speaker_id', 'next_speaker_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcae48",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e95fadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "combined_dataset = pd.merge(dataset, interaction_records, how='left', left_on=['project', 'meeting_number', 'speaker_number'], right_on=['project', 'meeting_number', 'speaker_id'])\n",
    "combined_dataset['count'] = combined_dataset['count'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d543f23",
   "metadata": {},
   "source": [
    "## Compute Network Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "517f433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'project', 'meeting_number', 'speaker_number', 'speech_frequency',\n",
       "       'total_words', 'duration', 'normalized_speech_frequency', 'speaker_id',\n",
       "       'next_speaker_id', 'count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2683c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the network density function\n",
    "def compute_density(G):\n",
    "    num_nodes = len(G)\n",
    "    if num_nodes < 2:\n",
    "        return 0\n",
    "    possible_edges = num_nodes * (num_nodes - 1)  # For directed graph\n",
    "    actual_edges = sum(1 for u, v, data in G.edges(data=True) if u != v and data['weight'] > 0)\n",
    "    return actual_edges / possible_edges\n",
    "\n",
    "# Compute density\n",
    "densities_project3 = []\n",
    "densities_project4 = []\n",
    "for df in dfs_project3:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']  # Self-interaction if last speaker\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            if G.has_edge(prev_speaker, next_speaker):\n",
    "                G[prev_speaker][next_speaker]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(prev_speaker, next_speaker, weight=1)\n",
    "    densities_project3.append(compute_density(G))\n",
    "\n",
    "for df in dfs_project4:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']  # Self-interaction if last speaker\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            if G.has_edge(prev_speaker, next_speaker):\n",
    "                G[prev_speaker][next_speaker]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(prev_speaker, next_speaker, weight=1)\n",
    "    densities_project4.append(compute_density(G))\n",
    "\n",
    "# Define the weighted density function\n",
    "def weighted_density(G):\n",
    "    if len(G) == 0:\n",
    "        return 0\n",
    "    total_weight = sum(data['weight'] for u, v, data in G.edges(data=True) if u != v)\n",
    "    num_nodes = len(G)\n",
    "    max_weight = max(data['weight'] for u, v, data in G.edges(data=True) if u != v)\n",
    "    possible_edges = num_nodes * (num_nodes - 1) # For directed graph\n",
    "    # possible_edges = num_nodes * (num_nodes - 1) * max_weight # For directed graph\n",
    "    return total_weight / possible_edges if possible_edges > 0 else 0\n",
    "\n",
    "# Compute weighted density\n",
    "weighted_densities_project3 = []\n",
    "weighted_densities_project4 = []\n",
    "for df in dfs_project3:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']  # Self-interaction if last speaker\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            if G.has_edge(prev_speaker, next_speaker):\n",
    "                G[prev_speaker][next_speaker]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(prev_speaker, next_speaker, weight=1)\n",
    "    weighted_densities_project3.append(weighted_density(G))\n",
    "\n",
    "for df in dfs_project4:\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']  # Self-interaction if last speaker\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            if G.has_edge(prev_speaker, next_speaker):\n",
    "                G[prev_speaker][next_speaker]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(prev_speaker, next_speaker, weight=1)\n",
    "    weighted_densities_project4.append(weighted_density(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cdce16",
   "metadata": {},
   "source": [
    "## Compute Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7d04611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define centrality measures function\n",
    "def compute_centralities(df):\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(df)):\n",
    "        prev_speaker = df.iloc[i]['Speaker']\n",
    "        if i < len(df) - 1:\n",
    "            next_speaker = df.iloc[i+1]['Speaker']\n",
    "        else:\n",
    "            next_speaker = df.iloc[i]['Speaker']  # Self-interaction if last speaker\n",
    "        if prev_speaker != next_speaker and df.iloc[i]['Text'].strip() != '':\n",
    "            if G.has_edge(prev_speaker, next_speaker):\n",
    "                G[prev_speaker][next_speaker]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(prev_speaker, next_speaker, weight=1)\n",
    "    if len(G) == 0:\n",
    "        centralities = {\n",
    "            'degree_centrality': {},\n",
    "            'indegree_centrality': {},\n",
    "            'outdegree_centrality': {},\n",
    "            'betweenness_centrality': {},\n",
    "            'closeness_centrality': {},\n",
    "            'eigenvector_centrality': {},\n",
    "            'pagerank': {}\n",
    "        }\n",
    "    else:\n",
    "        centralities = {\n",
    "            'degree_centrality': dict(G.degree(weight='weight')),\n",
    "            'indegree_centrality': dict(G.in_degree(weight='weight')),\n",
    "            'outdegree_centrality': dict(G.out_degree(weight='weight')),\n",
    "            'betweenness_centrality': betweenness_centrality(G, weight='weight'),\n",
    "            'closeness_centrality': closeness_centrality(G, distance='weight'),\n",
    "            'eigenvector_centrality': eigenvector_centrality(G, max_iter=500, weight='weight'),\n",
    "            'pagerank': nx.pagerank(G, weight='weight')\n",
    "        }\n",
    "    return centralities\n",
    "\n",
    "centralities_project3 = []\n",
    "centralities_project4 = []\n",
    "for df in dfs_project3:\n",
    "    centralities_project3.append(compute_centralities(df))\n",
    "\n",
    "for df in dfs_project4:\n",
    "    centralities_project4.append(compute_centralities(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402e853",
   "metadata": {},
   "source": [
    "## Add Centralities and Network Density to Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2dbb9733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\1019421709.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6666666666666666' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1) & (combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\1019421709.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.02830188679245283' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1) & (combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\1019421709.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6329854261349638' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1) & (combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\1019421709.py:13: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.33544592087540615' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1) & (combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\1019421709.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5833333333333334' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'network_density'] = density\n",
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\1019421709.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '10.666666666666666' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'weighted_network_density'] = weighted_density_value\n"
     ]
    }
   ],
   "source": [
    "# Add Centralities and Density to Combined Dataset\n",
    "for centrality_measure in ['degree_centrality', 'indegree_centrality', 'outdegree_centrality', 'betweenness_centrality', 'closeness_centrality', 'eigenvector_centrality', 'pagerank']:\n",
    "    combined_dataset[centrality_measure] = 0\n",
    "combined_dataset['network_density'] = 0\n",
    "combined_dataset['weighted_network_density'] = 0\n",
    "\n",
    "for i, df in enumerate(dfs_project3):\n",
    "    centralities = centralities_project3[i]\n",
    "    density = densities_project3[i]\n",
    "    weighted_density_value = weighted_densities_project3[i]\n",
    "    for centrality_measure, centrality_values in centralities.items():\n",
    "        for node, value in centrality_values.items():\n",
    "            combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1) & (combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'network_density'] = density\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'weighted_network_density'] = weighted_density_value\n",
    "\n",
    "for i, df in enumerate(dfs_project4):\n",
    "    centralities = centralities_project4[i]\n",
    "    density = densities_project4[i]\n",
    "    weighted_density_value = weighted_densities_project4[i]\n",
    "    for centrality_measure, centrality_values in centralities.items():\n",
    "        for node, value in centrality_values.items():\n",
    "            combined_dataset.loc[(combined_dataset['project'] == 4) & (combined_dataset['meeting_number'] == i + 1) & (combined_dataset['speaker_number'] == int(node.split('_')[1])), centrality_measure] = value\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (combined_dataset['meeting_number'] == i + 1), 'network_density'] = density\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (combined_dataset['meeting_number'] == i + 1), 'weighted_network_density'] = weighted_density_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8378a",
   "metadata": {},
   "source": [
    "## Compute Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c21474fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\316042812.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.40234374874267576' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'gini_coefficient'] = gini_project3[i]\n"
     ]
    }
   ],
   "source": [
    "# Define Gini coefficient function\n",
    "def gini_coefficient(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    if np.amin(x) < 0:\n",
    "        x -= np.amin(x)  # values cannot be negative\n",
    "    x += 0.0000001  # values cannot be 0\n",
    "    x = np.sort(x)  # values must be sorted\n",
    "    index = np.arange(1, x.shape[0] + 1)  # index per array element\n",
    "    n = x.shape[0]\n",
    "    return ((np.sum((2 * index - n - 1) * x)) / (n * np.sum(x)))\n",
    "\n",
    "# Compute Gini Coefficient for each meeting\n",
    "def compute_gini(df):\n",
    "    gini_values = []\n",
    "    meetings = df['meeting_number'].unique()\n",
    "    for meeting_number in meetings:\n",
    "        meeting_data = df[df['meeting_number'] == meeting_number]\n",
    "        interaction_counts = [meeting_data[(meeting_data['speaker_number'] == speaker) & (meeting_data['speaker_number'] != meeting_data['next_speaker_id'])]['count'].sum() for speaker in meeting_data['speaker_number'].unique()]\n",
    "        gini_values.append(gini_coefficient(interaction_counts))\n",
    "    return gini_values\n",
    "\n",
    "gini_project3 = compute_gini(combined_dataset[combined_dataset['project'] == 3])\n",
    "gini_project4 = compute_gini(combined_dataset[combined_dataset['project'] == 4])\n",
    "\n",
    "combined_dataset['gini_coefficient'] = 0\n",
    "\n",
    "for i in range(len(gini_project3)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'gini_coefficient'] = gini_project3[i]\n",
    "\n",
    "for i in range(len(gini_project4)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (combined_dataset['meeting_number'] == i + 1), 'gini_coefficient'] = gini_project4[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a496a",
   "metadata": {},
   "source": [
    "## Compute Interaction Equality Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f3d23047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunti\\AppData\\Local\\Temp\\ipykernel_70768\\83924318.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.27955363228759356' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'interaction_equality_index'] = equality_index_project3[i]\n"
     ]
    }
   ],
   "source": [
    "# Define Interaction Equality Index function\n",
    "def interaction_equality_index(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    mean_x = np.mean(x)\n",
    "    if mean_x == 0:\n",
    "        return 0\n",
    "    return 1 - (np.std(x) / mean_x)\n",
    "\n",
    "# Compute Interaction Equality Index for each meeting\n",
    "def compute_equality_index(df):\n",
    "    equality_index_values = []\n",
    "    meetings = df['meeting_number'].unique()\n",
    "    for meeting_number in meetings:\n",
    "        meeting_data = df[df['meeting_number'] == meeting_number]\n",
    "        interaction_counts = [meeting_data[(meeting_data['speaker_number'] == speaker) & (meeting_data['speaker_number'] != meeting_data['next_speaker_id'])]['count'].sum() for speaker in meeting_data['speaker_number'].unique()]\n",
    "        equality_index_values.append(interaction_equality_index(interaction_counts))\n",
    "    return equality_index_values\n",
    "\n",
    "equality_index_project3 = compute_equality_index(combined_dataset[combined_dataset['project'] == 3])\n",
    "equality_index_project4 = compute_equality_index(combined_dataset[combined_dataset['project'] == 4])\n",
    "\n",
    "combined_dataset['interaction_equality_index'] = 0\n",
    "\n",
    "for i in range(len(equality_index_project3)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 3) & (combined_dataset['meeting_number'] == i + 1), 'interaction_equality_index'] = equality_index_project3[i]\n",
    "\n",
    "for i in range(len(equality_index_project4)):\n",
    "    combined_dataset.loc[(combined_dataset['project'] == 4) & (combined_dataset['meeting_number'] == i + 1), 'interaction_equality_index'] = equality_index_project4[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbc62e",
   "metadata": {},
   "source": [
    "## Save Updated Combined Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "edaf33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "columns_order = [\n",
    "    'id', 'project', 'meeting_number', 'speaker_number', 'speech_frequency', 'total_words', 'duration', 'normalized_speech_frequency', 'speaker_id', 'next_speaker_id', 'count', 'network_density', 'weighted_network_density',\n",
    "    'gini_coefficient', 'interaction_equality_index', 'degree_centrality', 'indegree_centrality', 'outdegree_centrality', 'betweenness_centrality', 'closeness_centrality', 'eigenvector_centrality', 'pagerank'\n",
    "]\n",
    "combined_dataset = combined_dataset[columns_order]\n",
    "\n",
    "# Save the final dataset with centralities and density to a CSV file\n",
    "os.makedirs('data', exist_ok=True)\n",
    "combined_dataset.to_csv('data/dataset_collaboration.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
